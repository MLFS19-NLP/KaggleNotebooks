{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### The intention of this notebook is to provide the high level guidlines how to create an arbitrary dataset from twitter.\n#### As an example, a small dataset will be created, similar to the one used in [this challenge](https://www.kaggle.com/c/nlp-getting-started).","metadata":{}},{"cell_type":"markdown","source":"Note, you will not be run this notebook as is, unless you use your access tokens (`Access Token`, `Access Token Secret`, `Consumer Key`, `Consumer Secret`).\n\nTo access to twitter api, you will need to go through the registration proccess (free, but limited access) at https://developer.twitter.com/en.\n\nAfter that you will obtain the private keys&tokens.","metadata":{}},{"cell_type":"code","source":"!pip install twitter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T15:15:12.445901Z","iopub.execute_input":"2022-04-04T15:15:12.44651Z","iopub.status.idle":"2022-04-04T15:15:21.655348Z","shell.execute_reply.started":"2022-04-04T15:15:12.446469Z","shell.execute_reply":"2022-04-04T15:15:21.654242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.options.display.max_colwidth = 100\nimport twitter","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-04-04T15:15:29.275697Z","iopub.execute_input":"2022-04-04T15:15:29.27618Z","iopub.status.idle":"2022-04-04T15:15:29.295575Z","shell.execute_reply.started":"2022-04-04T15:15:29.276135Z","shell.execute_reply":"2022-04-04T15:15:29.294641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will start from the end: [here](https://www.kaggle.com/vstepanenko/disaster-tweets) is the dataset that has been created, following the steps described in this notebook.\nYou can find more info about the content and time of creation following that link.","metadata":{}},{"cell_type":"code","source":"disaster_tweets_df = pd.read_csv('../input/disaster-tweets/tweets.csv',\n                                 usecols=['keyword', 'location', 'text', 'target'])\ndisaster_tweets_df","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:15:33.595118Z","iopub.execute_input":"2022-04-04T15:15:33.595694Z","iopub.status.idle":"2022-04-04T15:15:33.689224Z","shell.execute_reply.started":"2022-04-04T15:15:33.595642Z","shell.execute_reply":"2022-04-04T15:15:33.688078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It has the same structure as in this competition and contains 11370 tweets.","metadata":{}},{"cell_type":"code","source":"disaster_tweets_df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:15:39.072593Z","iopub.execute_input":"2022-04-04T15:15:39.073199Z","iopub.status.idle":"2022-04-04T15:15:39.087511Z","shell.execute_reply.started":"2022-04-04T15:15:39.073159Z","shell.execute_reply":"2022-04-04T15:15:39.086269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now we start building a new smaller dataset from twitter from scratch","metadata":{}},{"cell_type":"code","source":"# Establishing the access to twitter api\n# Here I used my twitter credentials (they are now invalidated)\n# You will need to plug in yours.\n\n# VERY IMPORTANT!\n# Regenerate/revoke your keys, if you decide to publish your version of the notebook\n\ntw={\n    'Consumer Key': 'your_consumer_key',\n    'Consumer Secret': 'your_consumer_secret',\n    'Access Token': 'your_access_token',\n    'Access Token Secret': 'your_access_secret',\n   }\n\n\nauth = twitter.oauth.OAuth(tw['Access Token'],\n                           tw['Access Token Secret'],\n                           tw['Consumer Key'],\n                           tw['Consumer Secret'])\n\ntwitter_api = twitter.Twitter(auth=auth)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:17:13.80167Z","iopub.execute_input":"2022-04-04T15:17:13.802276Z","iopub.status.idle":"2022-04-04T15:17:13.810727Z","shell.execute_reply.started":"2022-04-04T15:17:13.802236Z","shell.execute_reply":"2022-04-04T15:17:13.809685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set the variable `q` to any disaster related keyword, or anything else of your interest.\n\nNote: The below cell will not run normally (`TwitterHTTPError` will be raised), unless you use your private keys.\n","metadata":{}},{"cell_type":"code","source":"q = 'covid19'\nnumber = 10 # number of tweets to query\nsearch_results = twitter_api.search.tweets(q=q, count=number)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:17:25.486862Z","iopub.execute_input":"2022-04-04T15:17:25.48737Z","iopub.status.idle":"2022-04-04T15:17:25.848786Z","shell.execute_reply.started":"2022-04-04T15:17:25.487339Z","shell.execute_reply":"2022-04-04T15:17:25.848038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The search_results contains two keys: `statuses` (lots of details about the tweet) and `search_metadata` (info about search parameters).\n\nFrom all available data in `statuses`, we will extract only `keyword`, `location`, `text`.","metadata":{}},{"cell_type":"code","source":"print(search_results.keys())\nstatuses = search_results['statuses']","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:17:32.950346Z","iopub.execute_input":"2022-04-04T15:17:32.950764Z","iopub.status.idle":"2022-04-04T15:17:32.95921Z","shell.execute_reply.started":"2022-04-04T15:17:32.950732Z","shell.execute_reply":"2022-04-04T15:17:32.957442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is how to extract `keyword`, `location`, `text` to replicate the structure of the dataset used in this competition.\nAlso we add column `target` and set it to `None`","metadata":{}},{"cell_type":"code","source":"example_df = pd.DataFrame(\n    data=[[q, s['user']['location'], s['text'], None] for s in statuses],\n    columns = ['keyword', 'location', 'text', 'target'],\n            )\n\nexample_df","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:18:05.306695Z","iopub.execute_input":"2022-04-04T15:18:05.307409Z","iopub.status.idle":"2022-04-04T15:18:05.326346Z","shell.execute_reply.started":"2022-04-04T15:18:05.307366Z","shell.execute_reply":"2022-04-04T15:18:05.324938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try some more keywords, and using `for-loop` to iterate over.\n\nAll keywords that could be found in `train` and `test` datasets (provided in the competition) have been reused to create [Disaster Tweets](https://www.kaggle.com/vstepanenko/disaster-tweets).","metadata":{}},{"cell_type":"code","source":"# Just as example, here I use four topics.\n# Feel free to complement/ammend the list with yours.\nkeywords=['war', 'lockdown', 'fire', 'crush']","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:18:26.437497Z","iopub.execute_input":"2022-04-04T15:18:26.437919Z","iopub.status.idle":"2022-04-04T15:18:26.443045Z","shell.execute_reply.started":"2022-04-04T15:18:26.437887Z","shell.execute_reply":"2022-04-04T15:18:26.441917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collect_tweets(keywords, count=10):\n    df = pd.DataFrame(columns=['keyword', 'location', 'text', 'target'])\n    for q in keywords:\n        search_results = twitter_api.search.tweets(q=q, count=count)\n        tmp_df = pd.DataFrame(\n            data=[[q, s['user']['location'], s['text'], None] for s in search_results['statuses']],\n             columns = ['keyword', 'location', 'text', 'target'],\n            )\n        df = df.append(tmp_df, ignore_index=True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:18:31.1286Z","iopub.execute_input":"2022-04-04T15:18:31.128965Z","iopub.status.idle":"2022-04-04T15:18:31.137218Z","shell.execute_reply.started":"2022-04-04T15:18:31.128932Z","shell.execute_reply":"2022-04-04T15:18:31.136215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We collect 20 tweets in total. 5 tweets over 4 topics.\ntweet_collection_df = collect_tweets(keywords, count=5)\ntweet_collection_df","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:18:38.56107Z","iopub.execute_input":"2022-04-04T15:18:38.56145Z","iopub.status.idle":"2022-04-04T15:18:40.521037Z","shell.execute_reply.started":"2022-04-04T15:18:38.561418Z","shell.execute_reply":"2022-04-04T15:18:40.519891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tweets have been collected into the dataframe. It is all done!\n\nYou may want to save your work to continue to tune the collected tweets off-line.","metadata":{}},{"cell_type":"code","source":"tweet_collection_df.to_csv('tweet_collection_df.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:18:57.959652Z","iopub.execute_input":"2022-04-04T15:18:57.960087Z","iopub.status.idle":"2022-04-04T15:18:58.183438Z","shell.execute_reply.started":"2022-04-04T15:18:57.960048Z","shell.execute_reply":"2022-04-04T15:18:58.18236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls tweet_collection_df.csv -l","metadata":{"execution":{"iopub.status.busy":"2022-04-04T15:19:02.163069Z","iopub.execute_input":"2022-04-04T15:19:02.163419Z","iopub.status.idle":"2022-04-04T15:19:02.921819Z","shell.execute_reply.started":"2022-04-04T15:19:02.163389Z","shell.execute_reply":"2022-04-04T15:19:02.920756Z"},"trusted":true},"execution_count":null,"outputs":[]}]}